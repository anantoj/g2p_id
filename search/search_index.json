{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home g2p ID: Indonesian Grapheme-to-Phoneme Converter This library is developed to convert Indonesian (Bahasa Indonesia) graphemes (words) to phonemes in IPA. We followed the methods and designs used in the English equivalent library, g2p . Installation pip install g2p_id_py How to Use from g2p_id import G2p texts = [ \"Apel itu berwarna merah.\" , \"Rahel bersekolah di Jakarta.\" , \"Mereka sedang bermain bola di lapangan.\" , ] g2p = G2p () for text in texts : print ( g2p ( text )) >> [[ 'a' , 'p' , '\u0259' , 'l' ], [ 'i' , 't' , 'u' ], [ 'b' , '\u0259' , 'r' , 'w' , 'a' , 'r' , 'n' , 'a' ], [ 'm' , 'e' , 'r' , 'a' , 'h' ], [ '.' ]] >> [[ 'r' , 'a' , 'h' , 'e' , 'l' ], [ 'b' , '\u0259' , 'r' , 's' , '\u0259' , 'k' , 'o' , 'l' , 'a' , 'h' ], [ 'd' , 'i' ], [ 'd\u0292' , 'a' , 'k' , 'a' , 'r' , 't' , 'a' ], [ '.' ]] >> [[ 'm' , '\u0259' , 'r' , 'e' , 'k' , 'a' ], [ 's' , '\u0259' , 'd' , 'a' , '\u014b' ], [ 'b' , '\u0259' , 'r' , 'm' , 'a' , 'i' , 'n' ], [ 'b' , 'o' , 'l' , 'a' ], [ 'd' , 'i' ], [ 'l' , 'a' , 'p' , 'a' , '\u014b' , 'a' , 'n' ], [ '.' ]] References @misc { g2pE2019 , author = {Park, Kyubyong & Kim, Jongseok} , title = {g2pE} , year = {2019} , publisher = {GitHub} , journal = {GitHub repository} , howpublished = {\\url{https://github.com/Kyubyong/g2p}} } @misc { TextProcessor2021 , author = {Cahya Wirawan} , title = {Text Processor} , year = {2021} , publisher = {GitHub} , journal = {GitHub repository} , howpublished = {\\url{https://github.com/cahya-wirawan/text_processor}} } Contributors","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#g2p-id-indonesian-grapheme-to-phoneme-converter","text":"This library is developed to convert Indonesian (Bahasa Indonesia) graphemes (words) to phonemes in IPA. We followed the methods and designs used in the English equivalent library, g2p .","title":"g2p ID: Indonesian Grapheme-to-Phoneme Converter"},{"location":"#installation","text":"pip install g2p_id_py","title":"Installation"},{"location":"#how-to-use","text":"from g2p_id import G2p texts = [ \"Apel itu berwarna merah.\" , \"Rahel bersekolah di Jakarta.\" , \"Mereka sedang bermain bola di lapangan.\" , ] g2p = G2p () for text in texts : print ( g2p ( text )) >> [[ 'a' , 'p' , '\u0259' , 'l' ], [ 'i' , 't' , 'u' ], [ 'b' , '\u0259' , 'r' , 'w' , 'a' , 'r' , 'n' , 'a' ], [ 'm' , 'e' , 'r' , 'a' , 'h' ], [ '.' ]] >> [[ 'r' , 'a' , 'h' , 'e' , 'l' ], [ 'b' , '\u0259' , 'r' , 's' , '\u0259' , 'k' , 'o' , 'l' , 'a' , 'h' ], [ 'd' , 'i' ], [ 'd\u0292' , 'a' , 'k' , 'a' , 'r' , 't' , 'a' ], [ '.' ]] >> [[ 'm' , '\u0259' , 'r' , 'e' , 'k' , 'a' ], [ 's' , '\u0259' , 'd' , 'a' , '\u014b' ], [ 'b' , '\u0259' , 'r' , 'm' , 'a' , 'i' , 'n' ], [ 'b' , 'o' , 'l' , 'a' ], [ 'd' , 'i' ], [ 'l' , 'a' , 'p' , 'a' , '\u014b' , 'a' , 'n' ], [ '.' ]]","title":"How to Use"},{"location":"#references","text":"@misc { g2pE2019 , author = {Park, Kyubyong & Kim, Jongseok} , title = {g2pE} , year = {2019} , publisher = {GitHub} , journal = {GitHub repository} , howpublished = {\\url{https://github.com/Kyubyong/g2p}} } @misc { TextProcessor2021 , author = {Cahya Wirawan} , title = {Text Processor} , year = {2021} , publisher = {GitHub} , journal = {GitHub repository} , howpublished = {\\url{https://github.com/cahya-wirawan/text_processor}} }","title":"References"},{"location":"#contributors","text":"","title":"Contributors"},{"location":"algorithm/","text":"Algorithm This is heavily inspired from the English g2p . Spells out arabic numbers and some currency symbols, e.g. Rp 200,000 -> dua ratus ribu rupiah . This is borrowed from Cahya's code . Attempts to retrieve the correct pronunciation for homographs based on their POS (part-of-speech) tags . Looks up a lexicon (pronunciation dictionary) for non-homographs. This list is originally from ipa-dict , and we later made a modified version. For OOVs, we predict their pronunciations using either a BERT model or an LSTM model . Phoneme and Grapheme Sets graphemes = [ 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' ] phonemes = [ 'a' , 'b' , 'd' , 'e' , 'f' , '\u0261' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'z' , '\u014b' , '\u0259' , '\u0272' , 't\u0283' , '\u0283' , 'd\u0292' , 'x' , '\u0294' ] Homographs Indonesian words (as far as we know) only have one case of homograph, that is, differing ways to pronounce the letter e . For instance, in the word apel (meaning: apple), the letter e is a mid central vowel \u0259 . On the other hand, the letter e in the word apel (meaning: going to a significant other's house; courting), is a closed-mid front unrounded vowel e . Sometimes, a word might have >1 e s pronounced in both ways, for instance, mereka (meaning: they) is pronounced as m\u0259reka . Because of this, there needs a way to disambiguate homographs, and in our case, we used their POS (part-of-speech) tags. However, this is not a foolproof method since homographs may even have the same POS tag. We are considering a contextual model to handle this better. OOV Prediction Initially, we relied on a sequence2sequence LSTM model for OOV (out-of-vocabulary) prediction. This was a natural choice given that it can \"automatically\" learn the rules of grapheme-to-phoneme conversion without having to determine the rules by hand. However, we soon noticed that despite its validation results, the model performed poorly on unseen words, especially on longer ones. We needed a more controllable model that makes predictions on necessary characters only. We ended up with a customized BERT that predicts the correct pronunciation of the letter e while keeping the rest of the string unchanged. We then apply a hand-written g2p conversion algorithm that handles the other characters. You can find more detail in this blog post . POS Tagging We trained an NLTK PerceptronTagger on the POSP dataset, which achieved 0.956 and 0.945 F1-score on the valid and test sets, respectively. Given its performance and speed, we decided to adopt this model as the POS tagger for the purpose of disambiguating homographs, which is just like the English g2p library. tag precision recall f1-score B-$$$ 1.000000 1.000000 1.000000 B-ADJ 0.904132 0.864139 0.883683 B-ADK 1.000000 0.986667 0.993289 B-ADV 0.966874 0.976987 0.971904 B-ART 0.988920 0.978082 0.983471 B-CCN 0.997934 0.997934 0.997934 B-CSN 0.986395 0.963455 0.974790 B-INT 1.000000 1.000000 1.000000 B-KUA 0.976744 0.976744 0.976744 B-NEG 0.992857 0.972028 0.982332 B-NNO 0.919917 0.941288 0.930480 B-NNP 0.917685 0.914703 0.916192 B-NUM 0.997358 0.954488 0.975452 B-PAR 1.000000 0.851064 0.919540 B-PPO 0.991206 0.991829 0.991517 B-PRI 1.000000 0.928571 0.962963 B-PRK 0.793103 0.851852 0.821429 B-PRN 0.988327 0.988327 0.988327 B-PRR 0.995465 1.000000 0.997727 B-SYM 0.999662 0.999323 0.999492 B-UNS 0.916667 0.733333 0.814815 B-VBE 1.000000 0.985714 0.992806 B-VBI 0.929119 0.877034 0.902326 B-VBL 1.000000 1.000000 1.000000 B-VBP 0.926606 0.933457 0.930018 B-VBT 0.939759 0.953333 0.946498 --------- --------- -------- -------- macro avg 0.966490 0.946937 0.955913 Attempts that Failed Parsed online PDF KBBI , but it turns out that it has very little phoneme descriptions. Scraped online Web KBBI , but it had a daily bandwidth which was too low to be used at this level. Potential Improvements There is a ton of room for improvements, both from the technical and the linguistic side of the approaches. Consider that a failure of one component may cascade to an incorrect conclusion. For instance, an incorrect POS tag can lead to the wrong phoneme, ditto for incorrect OOV prediction. We propose the following future improvements. Use a larger pronunciation lexicon instead of having to guess. Find a larger homograph list. Use contextual model instead of character-level RNNs. Consider hand-written rules for g2p conversion. Add to PyPI.","title":"Algorithm"},{"location":"algorithm/#algorithm","text":"This is heavily inspired from the English g2p . Spells out arabic numbers and some currency symbols, e.g. Rp 200,000 -> dua ratus ribu rupiah . This is borrowed from Cahya's code . Attempts to retrieve the correct pronunciation for homographs based on their POS (part-of-speech) tags . Looks up a lexicon (pronunciation dictionary) for non-homographs. This list is originally from ipa-dict , and we later made a modified version. For OOVs, we predict their pronunciations using either a BERT model or an LSTM model .","title":"Algorithm"},{"location":"algorithm/#phoneme-and-grapheme-sets","text":"graphemes = [ 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' ] phonemes = [ 'a' , 'b' , 'd' , 'e' , 'f' , '\u0261' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'z' , '\u014b' , '\u0259' , '\u0272' , 't\u0283' , '\u0283' , 'd\u0292' , 'x' , '\u0294' ]","title":"Phoneme and Grapheme Sets"},{"location":"algorithm/#homographs","text":"Indonesian words (as far as we know) only have one case of homograph, that is, differing ways to pronounce the letter e . For instance, in the word apel (meaning: apple), the letter e is a mid central vowel \u0259 . On the other hand, the letter e in the word apel (meaning: going to a significant other's house; courting), is a closed-mid front unrounded vowel e . Sometimes, a word might have >1 e s pronounced in both ways, for instance, mereka (meaning: they) is pronounced as m\u0259reka . Because of this, there needs a way to disambiguate homographs, and in our case, we used their POS (part-of-speech) tags. However, this is not a foolproof method since homographs may even have the same POS tag. We are considering a contextual model to handle this better.","title":"Homographs"},{"location":"algorithm/#oov-prediction","text":"Initially, we relied on a sequence2sequence LSTM model for OOV (out-of-vocabulary) prediction. This was a natural choice given that it can \"automatically\" learn the rules of grapheme-to-phoneme conversion without having to determine the rules by hand. However, we soon noticed that despite its validation results, the model performed poorly on unseen words, especially on longer ones. We needed a more controllable model that makes predictions on necessary characters only. We ended up with a customized BERT that predicts the correct pronunciation of the letter e while keeping the rest of the string unchanged. We then apply a hand-written g2p conversion algorithm that handles the other characters. You can find more detail in this blog post .","title":"OOV Prediction"},{"location":"algorithm/#pos-tagging","text":"We trained an NLTK PerceptronTagger on the POSP dataset, which achieved 0.956 and 0.945 F1-score on the valid and test sets, respectively. Given its performance and speed, we decided to adopt this model as the POS tagger for the purpose of disambiguating homographs, which is just like the English g2p library. tag precision recall f1-score B-$$$ 1.000000 1.000000 1.000000 B-ADJ 0.904132 0.864139 0.883683 B-ADK 1.000000 0.986667 0.993289 B-ADV 0.966874 0.976987 0.971904 B-ART 0.988920 0.978082 0.983471 B-CCN 0.997934 0.997934 0.997934 B-CSN 0.986395 0.963455 0.974790 B-INT 1.000000 1.000000 1.000000 B-KUA 0.976744 0.976744 0.976744 B-NEG 0.992857 0.972028 0.982332 B-NNO 0.919917 0.941288 0.930480 B-NNP 0.917685 0.914703 0.916192 B-NUM 0.997358 0.954488 0.975452 B-PAR 1.000000 0.851064 0.919540 B-PPO 0.991206 0.991829 0.991517 B-PRI 1.000000 0.928571 0.962963 B-PRK 0.793103 0.851852 0.821429 B-PRN 0.988327 0.988327 0.988327 B-PRR 0.995465 1.000000 0.997727 B-SYM 0.999662 0.999323 0.999492 B-UNS 0.916667 0.733333 0.814815 B-VBE 1.000000 0.985714 0.992806 B-VBI 0.929119 0.877034 0.902326 B-VBL 1.000000 1.000000 1.000000 B-VBP 0.926606 0.933457 0.930018 B-VBT 0.939759 0.953333 0.946498 --------- --------- -------- -------- macro avg 0.966490 0.946937 0.955913","title":"POS Tagging"},{"location":"algorithm/#attempts-that-failed","text":"Parsed online PDF KBBI , but it turns out that it has very little phoneme descriptions. Scraped online Web KBBI , but it had a daily bandwidth which was too low to be used at this level.","title":"Attempts that Failed"},{"location":"algorithm/#potential-improvements","text":"There is a ton of room for improvements, both from the technical and the linguistic side of the approaches. Consider that a failure of one component may cascade to an incorrect conclusion. For instance, an incorrect POS tag can lead to the wrong phoneme, ditto for incorrect OOV prediction. We propose the following future improvements. Use a larger pronunciation lexicon instead of having to guess. Find a larger homograph list. Use contextual model instead of character-level RNNs. Consider hand-written rules for g2p conversion. Add to PyPI.","title":"Potential Improvements"},{"location":"reference/bert/","text":"BERT g2p_id.bert.BERT Source code in g2p_id/bert.py class BERT : def __init__ ( self ): bert_model_path = os . path . join ( model_path , \"bert_mlm.onnx\" ) token2id = os . path . join ( model_path , \"token2id.json\" ) config_path = os . path . join ( model_path , \"config.json\" ) self . model = ort . InferenceSession ( bert_model_path ) self . token2id = json . load ( open ( token2id , encoding = \"utf-8\" )) self . id2token = { v : k for k , v in self . token2id . items ()} self . config = json . load ( open ( config_path , encoding = \"utf-8\" )) def predict ( self , text : str ) -> str : \"\"\"Performs BERT inference, predicting the correct phoneme for the letter `e`. Args: text (str): Word to predict from. Returns: str: Word after prediction. \"\"\" # `x` is currently OOV, we replace with text = text . replace ( \"x\" , \"ks\" ) # mask `e`'s text = \" \" . join ([ c if c != \"e\" else \"[mask]\" for c in text ]) # tokenize and pad to max length tokens = [ self . token2id [ c ] for c in text . split ()] padding = [ self . token2id [ self . config [ \"pad_token\" ]] for _ in range ( self . config [ \"max_seq_length\" ] - len ( tokens )) ] tokens = tokens + padding input_ids = np . array ([ tokens ], dtype = \"int64\" ) inputs = { \"input_1\" : input_ids } prediction = self . model . run ( None , inputs ) # find masked idx token mask_token_id = self . token2id [ self . config [ \"mask_token\" ]] masked_index = np . where ( input_ids == mask_token_id )[ 1 ] # get prediction at masked indices mask_prediction = prediction [ 0 ][ 0 ][ masked_index ] predicted_ids = np . argmax ( mask_prediction , axis = 1 ) # replace mask with predicted token for i , idx in enumerate ( masked_index ): tokens [ idx ] = predicted_ids [ i ] return \"\" . join ([ self . id2token [ t ] for t in tokens if t != 0 ]) predict ( self , text ) Performs BERT inference, predicting the correct phoneme for the letter e . Parameters: Name Type Description Default text str Word to predict from. required Returns: Type Description str Word after prediction. Source code in g2p_id/bert.py def predict ( self , text : str ) -> str : \"\"\"Performs BERT inference, predicting the correct phoneme for the letter `e`. Args: text (str): Word to predict from. Returns: str: Word after prediction. \"\"\" # `x` is currently OOV, we replace with text = text . replace ( \"x\" , \"ks\" ) # mask `e`'s text = \" \" . join ([ c if c != \"e\" else \"[mask]\" for c in text ]) # tokenize and pad to max length tokens = [ self . token2id [ c ] for c in text . split ()] padding = [ self . token2id [ self . config [ \"pad_token\" ]] for _ in range ( self . config [ \"max_seq_length\" ] - len ( tokens )) ] tokens = tokens + padding input_ids = np . array ([ tokens ], dtype = \"int64\" ) inputs = { \"input_1\" : input_ids } prediction = self . model . run ( None , inputs ) # find masked idx token mask_token_id = self . token2id [ self . config [ \"mask_token\" ]] masked_index = np . where ( input_ids == mask_token_id )[ 1 ] # get prediction at masked indices mask_prediction = prediction [ 0 ][ 0 ][ masked_index ] predicted_ids = np . argmax ( mask_prediction , axis = 1 ) # replace mask with predicted token for i , idx in enumerate ( masked_index ): tokens [ idx ] = predicted_ids [ i ] return \"\" . join ([ self . id2token [ t ] for t in tokens if t != 0 ]) Usage texts = [ \"mengembangkannya\" , \"merdeka\" , \"pecel\" , \"lele\" ] bert = BERT () for text in texts : print ( bert . predict ( text )) >> m\u0259ng\u0259mbangkannya >> m\u0259rdeka >> p\u0259cel >> lele","title":"BERT"},{"location":"reference/bert/#bert","text":"","title":"BERT"},{"location":"reference/bert/#g2p_id.bert.BERT","text":"Source code in g2p_id/bert.py class BERT : def __init__ ( self ): bert_model_path = os . path . join ( model_path , \"bert_mlm.onnx\" ) token2id = os . path . join ( model_path , \"token2id.json\" ) config_path = os . path . join ( model_path , \"config.json\" ) self . model = ort . InferenceSession ( bert_model_path ) self . token2id = json . load ( open ( token2id , encoding = \"utf-8\" )) self . id2token = { v : k for k , v in self . token2id . items ()} self . config = json . load ( open ( config_path , encoding = \"utf-8\" )) def predict ( self , text : str ) -> str : \"\"\"Performs BERT inference, predicting the correct phoneme for the letter `e`. Args: text (str): Word to predict from. Returns: str: Word after prediction. \"\"\" # `x` is currently OOV, we replace with text = text . replace ( \"x\" , \"ks\" ) # mask `e`'s text = \" \" . join ([ c if c != \"e\" else \"[mask]\" for c in text ]) # tokenize and pad to max length tokens = [ self . token2id [ c ] for c in text . split ()] padding = [ self . token2id [ self . config [ \"pad_token\" ]] for _ in range ( self . config [ \"max_seq_length\" ] - len ( tokens )) ] tokens = tokens + padding input_ids = np . array ([ tokens ], dtype = \"int64\" ) inputs = { \"input_1\" : input_ids } prediction = self . model . run ( None , inputs ) # find masked idx token mask_token_id = self . token2id [ self . config [ \"mask_token\" ]] masked_index = np . where ( input_ids == mask_token_id )[ 1 ] # get prediction at masked indices mask_prediction = prediction [ 0 ][ 0 ][ masked_index ] predicted_ids = np . argmax ( mask_prediction , axis = 1 ) # replace mask with predicted token for i , idx in enumerate ( masked_index ): tokens [ idx ] = predicted_ids [ i ] return \"\" . join ([ self . id2token [ t ] for t in tokens if t != 0 ])","title":"BERT"},{"location":"reference/bert/#g2p_id.bert.BERT.predict","text":"Performs BERT inference, predicting the correct phoneme for the letter e . Parameters: Name Type Description Default text str Word to predict from. required Returns: Type Description str Word after prediction. Source code in g2p_id/bert.py def predict ( self , text : str ) -> str : \"\"\"Performs BERT inference, predicting the correct phoneme for the letter `e`. Args: text (str): Word to predict from. Returns: str: Word after prediction. \"\"\" # `x` is currently OOV, we replace with text = text . replace ( \"x\" , \"ks\" ) # mask `e`'s text = \" \" . join ([ c if c != \"e\" else \"[mask]\" for c in text ]) # tokenize and pad to max length tokens = [ self . token2id [ c ] for c in text . split ()] padding = [ self . token2id [ self . config [ \"pad_token\" ]] for _ in range ( self . config [ \"max_seq_length\" ] - len ( tokens )) ] tokens = tokens + padding input_ids = np . array ([ tokens ], dtype = \"int64\" ) inputs = { \"input_1\" : input_ids } prediction = self . model . run ( None , inputs ) # find masked idx token mask_token_id = self . token2id [ self . config [ \"mask_token\" ]] masked_index = np . where ( input_ids == mask_token_id )[ 1 ] # get prediction at masked indices mask_prediction = prediction [ 0 ][ 0 ][ masked_index ] predicted_ids = np . argmax ( mask_prediction , axis = 1 ) # replace mask with predicted token for i , idx in enumerate ( masked_index ): tokens [ idx ] = predicted_ids [ i ] return \"\" . join ([ self . id2token [ t ] for t in tokens if t != 0 ])","title":"predict()"},{"location":"reference/bert/#usage","text":"texts = [ \"mengembangkannya\" , \"merdeka\" , \"pecel\" , \"lele\" ] bert = BERT () for text in texts : print ( bert . predict ( text )) >> m\u0259ng\u0259mbangkannya >> m\u0259rdeka >> p\u0259cel >> lele","title":"Usage"},{"location":"reference/g2p/","text":"G2p g2p_id.g2p.G2p Source code in g2p_id/g2p.py class G2p : def __init__ ( self , model_type = \"BERT\" ): \"\"\"Constructor for G2p. Args: model_type (str, optional): Type of neural network to use for prediction. Choices are \"LSTM\" or \"BERT\". Defaults to \"BERT\". \"\"\" self . homograph2features = construct_homographs_dictionary () self . lexicon2features = construct_lexicon_dictionary () self . normalizer = TextProcessor () self . tagger = PerceptronTagger ( load = False ) tagger_path = os . path . join ( resources_path , \"id_posp_tagger.pickle\" ) self . tagger . load ( \"file://\" + tagger_path ) self . model = BERT () if model_type == \"BERT\" else LSTM () self . pos_dict = { \"N\" : [ \"B-NNO\" , \"B-NNP\" , \"B-PRN\" , \"B-PRN\" , \"B-PRK\" ], \"V\" : [ \"B-VBI\" , \"B-VBT\" , \"B-VBP\" , \"B-VBL\" , \"B-VBE\" ], \"A\" : [ \"B-ADJ\" ], \"P\" : [ \"B-PAR\" ], } def _preprocess ( self , text : str ) -> str : \"\"\"Performs preprocessing. (1) Adds spaces in between tokens (2) Normalizes unicode and accents (3) Normalizes numbers (4) Lower case texts (5) Removes unwanted tokens Arguments: text (str): Text to preprocess. Returns: str: Preprocessed text. \"\"\" text = text . replace ( \"-\" , \" \" ) text = \" \" . join ( word_tokenize ( text )) text = unicode ( text ) text = \"\" . join ( char for char in unicodedata . normalize ( \"NFD\" , text ) if unicodedata . category ( char ) != \"Mn\" ) text = self . normalizer . normalize ( text ) . strip () text = text . lower () text = re . sub ( r \"[^ a-z'.,?!\\-]\" , \"\" , text ) return text def _rule_based_g2p ( self , text : str ) -> str : \"\"\"Applies rule-based Indonesian grapheme2phoneme conversion. Args: text (str): Grapheme text to convert to phoneme. Returns: str: Phoneme string. \"\"\" _PHONETIC_MAPPING = { \"'\" : \"\u0294\" , \"g\" : \"\u0261\" , \"q\" : \"k\" , \"j\" : \"d\u0292\" , \"y\" : \"j\" , \"x\" : \"ks\" , \"c\" : \"t\u0283\" , \"kh\" : \"x\" , \"ny\" : \"\u0272\" , \"ng\" : \"\u014b\" , \"sy\" : \"\u0283\" , \"aa\" : \"a\u0294a\" , \"ii\" : \"i\u0294i\" , \"oo\" : \"o\u0294o\" , \"\u0259\u0259\" : \"\u0259\u0294\u0259\" , \"uu\" : \"u\u0294u\" , } _CONSONANTS = \"bdjklmnprstw\u0272\" if text . endswith ( \"k\" ): text = text [: - 1 ] + \"\u0294\" if text . startswith ( \"x\" ): text = \"s\" + text [ 1 :] for g , p in _PHONETIC_MAPPING . items (): text = text . replace ( g , p ) for c in _CONSONANTS : text = text . replace ( f \"k { c } \" , f \"\u0294 { c } \" ) phonemes = [ list ( phn ) if phn != \"d\u0292\" and phn != \"t\u0283\" else [ phn ] for phn in re . split ( \"(t\u0283|d\u0292)\" , text ) ] return \" \" . join ([ p for phn in phonemes for p in phn ]) def __call__ ( self , text : str ) -> List [ str ]: \"\"\"Grapheme-to-phoneme converter. 1. Preprocess and normalize text 2. Word tokenizes text 3. Predict POS for every word 4. If word is non-alphabetic, add to list (i.e. punctuation) 5. If word is a homograph, check POS and use matching word's phonemes 6. If word is a non-homograph, lookup lexicon 7. Otherwise, predict with a neural network Args: text (str): Grapheme text to convert to phoneme. Returns: List[str]: List of strings in phonemes. \"\"\" text = self . _preprocess ( text ) words = word_tokenize ( text ) tokens = self . tagger . tag ( words ) prons = [] for word , pos in tokens : pron = \"\" if re . search ( \"[a-z]\" , word ) is None : # non-alphabetic pron = word elif word in self . homograph2features : # check if homograph pron1 , pron2 , pos1 , _ = self . homograph2features [ word ] # check for the matching POS if pos in self . pos_dict [ pos1 ]: pron = pron1 else : pron = pron2 elif word in self . lexicon2features : # non-homographs pron = self . lexicon2features [ word ] else : # predict for OOV pron = self . model . predict ( word ) if isinstance ( self . model , BERT ): pron = self . _rule_based_g2p ( pron ) prons . append ( pron . split ()) return prons __call__ ( self , text ) special Grapheme-to-phoneme converter. Preprocess and normalize text Word tokenizes text Predict POS for every word If word is non-alphabetic, add to list (i.e. punctuation) If word is a homograph, check POS and use matching word's phonemes If word is a non-homograph, lookup lexicon Otherwise, predict with a neural network Parameters: Name Type Description Default text str Grapheme text to convert to phoneme. required Returns: Type Description List[str] List of strings in phonemes. Source code in g2p_id/g2p.py def __call__ ( self , text : str ) -> List [ str ]: \"\"\"Grapheme-to-phoneme converter. 1. Preprocess and normalize text 2. Word tokenizes text 3. Predict POS for every word 4. If word is non-alphabetic, add to list (i.e. punctuation) 5. If word is a homograph, check POS and use matching word's phonemes 6. If word is a non-homograph, lookup lexicon 7. Otherwise, predict with a neural network Args: text (str): Grapheme text to convert to phoneme. Returns: List[str]: List of strings in phonemes. \"\"\" text = self . _preprocess ( text ) words = word_tokenize ( text ) tokens = self . tagger . tag ( words ) prons = [] for word , pos in tokens : pron = \"\" if re . search ( \"[a-z]\" , word ) is None : # non-alphabetic pron = word elif word in self . homograph2features : # check if homograph pron1 , pron2 , pos1 , _ = self . homograph2features [ word ] # check for the matching POS if pos in self . pos_dict [ pos1 ]: pron = pron1 else : pron = pron2 elif word in self . lexicon2features : # non-homographs pron = self . lexicon2features [ word ] else : # predict for OOV pron = self . model . predict ( word ) if isinstance ( self . model , BERT ): pron = self . _rule_based_g2p ( pron ) prons . append ( pron . split ()) return prons __init__ ( self , model_type = 'BERT' ) special Constructor for G2p. Parameters: Name Type Description Default model_type str Type of neural network to use for prediction. Choices are \"LSTM\" or \"BERT\". Defaults to \"BERT\". 'BERT' Source code in g2p_id/g2p.py def __init__ ( self , model_type = \"BERT\" ): \"\"\"Constructor for G2p. Args: model_type (str, optional): Type of neural network to use for prediction. Choices are \"LSTM\" or \"BERT\". Defaults to \"BERT\". \"\"\" self . homograph2features = construct_homographs_dictionary () self . lexicon2features = construct_lexicon_dictionary () self . normalizer = TextProcessor () self . tagger = PerceptronTagger ( load = False ) tagger_path = os . path . join ( resources_path , \"id_posp_tagger.pickle\" ) self . tagger . load ( \"file://\" + tagger_path ) self . model = BERT () if model_type == \"BERT\" else LSTM () self . pos_dict = { \"N\" : [ \"B-NNO\" , \"B-NNP\" , \"B-PRN\" , \"B-PRN\" , \"B-PRK\" ], \"V\" : [ \"B-VBI\" , \"B-VBT\" , \"B-VBP\" , \"B-VBL\" , \"B-VBE\" ], \"A\" : [ \"B-ADJ\" ], \"P\" : [ \"B-PAR\" ], } Usage texts = [ \"Apel itu berwarna merah.\" , \"Rahel bersekolah di Jakarta.\" , \"Mereka sedang bermain bola di lapangan.\" , ] g2p = G2p ( model_type = \"BERT\" ) for text in texts : print ( g2p ( text )) >> [[ 'a' , 'p' , '\u0259' , 'l' ], [ 'i' , 't' , 'u' ], [ 'b' , '\u0259' , 'r' , 'w' , 'a' , 'r' , 'n' , 'a' ], [ 'm' , 'e' , 'r' , 'a' , 'h' ], [ '.' ]] >> [[ 'r' , 'a' , 'h' , 'e' , 'l' ], [ 'b' , '\u0259' , 'r' , 's' , '\u0259' , 'k' , 'o' , 'l' , 'a' , 'h' ], [ 'd' , 'i' ], [ 'd\u0292' , 'a' , 'k' , 'a' , 'r' , 't' , 'a' ], [ '.' ]] >> [[ 'm' , '\u0259' , 'r' , 'e' , 'k' , 'a' ], [ 's' , '\u0259' , 'd' , 'a' , '\u014b' ], [ 'b' , '\u0259' , 'r' , 'm' , 'a' , 'i' , 'n' ], [ 'b' , 'o' , 'l' , 'a' ], [ 'd' , 'i' ], [ 'l' , 'a' , 'p' , 'a' , '\u014b' , 'a' , 'n' ], [ '.' ]]","title":"G2p"},{"location":"reference/g2p/#g2p","text":"","title":"G2p"},{"location":"reference/g2p/#g2p_id.g2p.G2p","text":"Source code in g2p_id/g2p.py class G2p : def __init__ ( self , model_type = \"BERT\" ): \"\"\"Constructor for G2p. Args: model_type (str, optional): Type of neural network to use for prediction. Choices are \"LSTM\" or \"BERT\". Defaults to \"BERT\". \"\"\" self . homograph2features = construct_homographs_dictionary () self . lexicon2features = construct_lexicon_dictionary () self . normalizer = TextProcessor () self . tagger = PerceptronTagger ( load = False ) tagger_path = os . path . join ( resources_path , \"id_posp_tagger.pickle\" ) self . tagger . load ( \"file://\" + tagger_path ) self . model = BERT () if model_type == \"BERT\" else LSTM () self . pos_dict = { \"N\" : [ \"B-NNO\" , \"B-NNP\" , \"B-PRN\" , \"B-PRN\" , \"B-PRK\" ], \"V\" : [ \"B-VBI\" , \"B-VBT\" , \"B-VBP\" , \"B-VBL\" , \"B-VBE\" ], \"A\" : [ \"B-ADJ\" ], \"P\" : [ \"B-PAR\" ], } def _preprocess ( self , text : str ) -> str : \"\"\"Performs preprocessing. (1) Adds spaces in between tokens (2) Normalizes unicode and accents (3) Normalizes numbers (4) Lower case texts (5) Removes unwanted tokens Arguments: text (str): Text to preprocess. Returns: str: Preprocessed text. \"\"\" text = text . replace ( \"-\" , \" \" ) text = \" \" . join ( word_tokenize ( text )) text = unicode ( text ) text = \"\" . join ( char for char in unicodedata . normalize ( \"NFD\" , text ) if unicodedata . category ( char ) != \"Mn\" ) text = self . normalizer . normalize ( text ) . strip () text = text . lower () text = re . sub ( r \"[^ a-z'.,?!\\-]\" , \"\" , text ) return text def _rule_based_g2p ( self , text : str ) -> str : \"\"\"Applies rule-based Indonesian grapheme2phoneme conversion. Args: text (str): Grapheme text to convert to phoneme. Returns: str: Phoneme string. \"\"\" _PHONETIC_MAPPING = { \"'\" : \"\u0294\" , \"g\" : \"\u0261\" , \"q\" : \"k\" , \"j\" : \"d\u0292\" , \"y\" : \"j\" , \"x\" : \"ks\" , \"c\" : \"t\u0283\" , \"kh\" : \"x\" , \"ny\" : \"\u0272\" , \"ng\" : \"\u014b\" , \"sy\" : \"\u0283\" , \"aa\" : \"a\u0294a\" , \"ii\" : \"i\u0294i\" , \"oo\" : \"o\u0294o\" , \"\u0259\u0259\" : \"\u0259\u0294\u0259\" , \"uu\" : \"u\u0294u\" , } _CONSONANTS = \"bdjklmnprstw\u0272\" if text . endswith ( \"k\" ): text = text [: - 1 ] + \"\u0294\" if text . startswith ( \"x\" ): text = \"s\" + text [ 1 :] for g , p in _PHONETIC_MAPPING . items (): text = text . replace ( g , p ) for c in _CONSONANTS : text = text . replace ( f \"k { c } \" , f \"\u0294 { c } \" ) phonemes = [ list ( phn ) if phn != \"d\u0292\" and phn != \"t\u0283\" else [ phn ] for phn in re . split ( \"(t\u0283|d\u0292)\" , text ) ] return \" \" . join ([ p for phn in phonemes for p in phn ]) def __call__ ( self , text : str ) -> List [ str ]: \"\"\"Grapheme-to-phoneme converter. 1. Preprocess and normalize text 2. Word tokenizes text 3. Predict POS for every word 4. If word is non-alphabetic, add to list (i.e. punctuation) 5. If word is a homograph, check POS and use matching word's phonemes 6. If word is a non-homograph, lookup lexicon 7. Otherwise, predict with a neural network Args: text (str): Grapheme text to convert to phoneme. Returns: List[str]: List of strings in phonemes. \"\"\" text = self . _preprocess ( text ) words = word_tokenize ( text ) tokens = self . tagger . tag ( words ) prons = [] for word , pos in tokens : pron = \"\" if re . search ( \"[a-z]\" , word ) is None : # non-alphabetic pron = word elif word in self . homograph2features : # check if homograph pron1 , pron2 , pos1 , _ = self . homograph2features [ word ] # check for the matching POS if pos in self . pos_dict [ pos1 ]: pron = pron1 else : pron = pron2 elif word in self . lexicon2features : # non-homographs pron = self . lexicon2features [ word ] else : # predict for OOV pron = self . model . predict ( word ) if isinstance ( self . model , BERT ): pron = self . _rule_based_g2p ( pron ) prons . append ( pron . split ()) return prons","title":"G2p"},{"location":"reference/g2p/#g2p_id.g2p.G2p.__call__","text":"Grapheme-to-phoneme converter. Preprocess and normalize text Word tokenizes text Predict POS for every word If word is non-alphabetic, add to list (i.e. punctuation) If word is a homograph, check POS and use matching word's phonemes If word is a non-homograph, lookup lexicon Otherwise, predict with a neural network Parameters: Name Type Description Default text str Grapheme text to convert to phoneme. required Returns: Type Description List[str] List of strings in phonemes. Source code in g2p_id/g2p.py def __call__ ( self , text : str ) -> List [ str ]: \"\"\"Grapheme-to-phoneme converter. 1. Preprocess and normalize text 2. Word tokenizes text 3. Predict POS for every word 4. If word is non-alphabetic, add to list (i.e. punctuation) 5. If word is a homograph, check POS and use matching word's phonemes 6. If word is a non-homograph, lookup lexicon 7. Otherwise, predict with a neural network Args: text (str): Grapheme text to convert to phoneme. Returns: List[str]: List of strings in phonemes. \"\"\" text = self . _preprocess ( text ) words = word_tokenize ( text ) tokens = self . tagger . tag ( words ) prons = [] for word , pos in tokens : pron = \"\" if re . search ( \"[a-z]\" , word ) is None : # non-alphabetic pron = word elif word in self . homograph2features : # check if homograph pron1 , pron2 , pos1 , _ = self . homograph2features [ word ] # check for the matching POS if pos in self . pos_dict [ pos1 ]: pron = pron1 else : pron = pron2 elif word in self . lexicon2features : # non-homographs pron = self . lexicon2features [ word ] else : # predict for OOV pron = self . model . predict ( word ) if isinstance ( self . model , BERT ): pron = self . _rule_based_g2p ( pron ) prons . append ( pron . split ()) return prons","title":"__call__()"},{"location":"reference/g2p/#g2p_id.g2p.G2p.__init__","text":"Constructor for G2p. Parameters: Name Type Description Default model_type str Type of neural network to use for prediction. Choices are \"LSTM\" or \"BERT\". Defaults to \"BERT\". 'BERT' Source code in g2p_id/g2p.py def __init__ ( self , model_type = \"BERT\" ): \"\"\"Constructor for G2p. Args: model_type (str, optional): Type of neural network to use for prediction. Choices are \"LSTM\" or \"BERT\". Defaults to \"BERT\". \"\"\" self . homograph2features = construct_homographs_dictionary () self . lexicon2features = construct_lexicon_dictionary () self . normalizer = TextProcessor () self . tagger = PerceptronTagger ( load = False ) tagger_path = os . path . join ( resources_path , \"id_posp_tagger.pickle\" ) self . tagger . load ( \"file://\" + tagger_path ) self . model = BERT () if model_type == \"BERT\" else LSTM () self . pos_dict = { \"N\" : [ \"B-NNO\" , \"B-NNP\" , \"B-PRN\" , \"B-PRN\" , \"B-PRK\" ], \"V\" : [ \"B-VBI\" , \"B-VBT\" , \"B-VBP\" , \"B-VBL\" , \"B-VBE\" ], \"A\" : [ \"B-ADJ\" ], \"P\" : [ \"B-PAR\" ], }","title":"__init__()"},{"location":"reference/g2p/#usage","text":"texts = [ \"Apel itu berwarna merah.\" , \"Rahel bersekolah di Jakarta.\" , \"Mereka sedang bermain bola di lapangan.\" , ] g2p = G2p ( model_type = \"BERT\" ) for text in texts : print ( g2p ( text )) >> [[ 'a' , 'p' , '\u0259' , 'l' ], [ 'i' , 't' , 'u' ], [ 'b' , '\u0259' , 'r' , 'w' , 'a' , 'r' , 'n' , 'a' ], [ 'm' , 'e' , 'r' , 'a' , 'h' ], [ '.' ]] >> [[ 'r' , 'a' , 'h' , 'e' , 'l' ], [ 'b' , '\u0259' , 'r' , 's' , '\u0259' , 'k' , 'o' , 'l' , 'a' , 'h' ], [ 'd' , 'i' ], [ 'd\u0292' , 'a' , 'k' , 'a' , 'r' , 't' , 'a' ], [ '.' ]] >> [[ 'm' , '\u0259' , 'r' , 'e' , 'k' , 'a' ], [ 's' , '\u0259' , 'd' , 'a' , '\u014b' ], [ 'b' , '\u0259' , 'r' , 'm' , 'a' , 'i' , 'n' ], [ 'b' , 'o' , 'l' , 'a' ], [ 'd' , 'i' ], [ 'l' , 'a' , 'p' , 'a' , '\u014b' , 'a' , 'n' ], [ '.' ]]","title":"Usage"},{"location":"reference/lstm/","text":"LSTM g2p_id.lstm.LSTM Source code in g2p_id/lstm.py class LSTM : def __init__ ( self ): encoder_model_path = os . path . join ( model_path , \"encoder_model.onnx\" ) decoder_model_path = os . path . join ( model_path , \"decoder_model.onnx\" ) g2id_path = os . path . join ( model_path , \"g2id.json\" ) p2id_path = os . path . join ( model_path , \"p2id.json\" ) config_path = os . path . join ( model_path , \"config.json\" ) self . encoder = ort . InferenceSession ( encoder_model_path ) self . decoder = ort . InferenceSession ( decoder_model_path ) self . g2id = json . load ( open ( g2id_path , encoding = \"utf-8\" )) self . p2id = json . load ( open ( p2id_path , encoding = \"utf-8\" )) self . id2p = { v : k for k , v in self . p2id . items ()} self . config = json . load ( open ( config_path , encoding = \"utf-8\" )) def predict ( self , text : str ) -> str : \"\"\"Performs LSTM inference, predicting phonemes of a given word. Args: text (str): Word to convert to phonemes. Returns: str: Word in phonemes. \"\"\" input_seq = np . zeros ( ( 1 , self . config [ \"max_encoder_seq_length\" ], self . config [ \"num_encoder_tokens\" ], ), dtype = \"float32\" , ) for t , char in enumerate ( text ): input_seq [ 0 , t , self . g2id [ char ]] = 1.0 input_seq [ 0 , t + 1 :, self . g2id [ self . config [ \"pad_token\" ]]] = 1.0 encoder_inputs = { \"input_1\" : input_seq } states_value = self . encoder . run ( None , encoder_inputs ) target_seq = np . zeros ( ( 1 , 1 , self . config [ \"num_decoder_tokens\" ]), dtype = \"float32\" ) target_seq [ 0 , 0 , self . p2id [ self . config [ \"bos_token\" ]]] = 1.0 stop_condition = False decoded_sentence = \"\" while not stop_condition : decoder_inputs = { \"input_2\" : target_seq , \"input_3\" : states_value [ 0 ], \"input_4\" : states_value [ 1 ], } output_tokens , h , c = self . decoder . run ( None , decoder_inputs ) sampled_token_index = np . argmax ( output_tokens [ 0 , - 1 , :]) sampled_char = self . id2p [ sampled_token_index ] decoded_sentence += sampled_char if ( sampled_char == self . config [ \"eos_token\" ] or len ( decoded_sentence ) > self . config [ \"max_decoder_seq_length\" ] ): stop_condition = True target_seq = np . zeros ( ( 1 , 1 , self . config [ \"num_decoder_tokens\" ]), dtype = \"float32\" ) target_seq [ 0 , 0 , sampled_token_index ] = 1.0 states_value = [ h , c ] return decoded_sentence . replace ( self . config [ \"eos_token\" ], \"\" ) predict ( self , text ) Performs LSTM inference, predicting phonemes of a given word. Parameters: Name Type Description Default text str Word to convert to phonemes. required Returns: Type Description str Word in phonemes. Source code in g2p_id/lstm.py def predict ( self , text : str ) -> str : \"\"\"Performs LSTM inference, predicting phonemes of a given word. Args: text (str): Word to convert to phonemes. Returns: str: Word in phonemes. \"\"\" input_seq = np . zeros ( ( 1 , self . config [ \"max_encoder_seq_length\" ], self . config [ \"num_encoder_tokens\" ], ), dtype = \"float32\" , ) for t , char in enumerate ( text ): input_seq [ 0 , t , self . g2id [ char ]] = 1.0 input_seq [ 0 , t + 1 :, self . g2id [ self . config [ \"pad_token\" ]]] = 1.0 encoder_inputs = { \"input_1\" : input_seq } states_value = self . encoder . run ( None , encoder_inputs ) target_seq = np . zeros ( ( 1 , 1 , self . config [ \"num_decoder_tokens\" ]), dtype = \"float32\" ) target_seq [ 0 , 0 , self . p2id [ self . config [ \"bos_token\" ]]] = 1.0 stop_condition = False decoded_sentence = \"\" while not stop_condition : decoder_inputs = { \"input_2\" : target_seq , \"input_3\" : states_value [ 0 ], \"input_4\" : states_value [ 1 ], } output_tokens , h , c = self . decoder . run ( None , decoder_inputs ) sampled_token_index = np . argmax ( output_tokens [ 0 , - 1 , :]) sampled_char = self . id2p [ sampled_token_index ] decoded_sentence += sampled_char if ( sampled_char == self . config [ \"eos_token\" ] or len ( decoded_sentence ) > self . config [ \"max_decoder_seq_length\" ] ): stop_condition = True target_seq = np . zeros ( ( 1 , 1 , self . config [ \"num_decoder_tokens\" ]), dtype = \"float32\" ) target_seq [ 0 , 0 , sampled_token_index ] = 1.0 states_value = [ h , c ] return decoded_sentence . replace ( self . config [ \"eos_token\" ], \"\" ) Usage texts = [ \"mengembangkannya\" , \"merdeka\" , \"pecel\" , \"lele\" ] lstm = LSTM () for text in texts : print ( lstm . predict ( text )) >> m\u0259\u014b\u0259mba\u014bkan\u0272a >> m\u0259rdeka >> p\u0259t\u0283\u0259l >> lele","title":"LSTM"},{"location":"reference/lstm/#lstm","text":"","title":"LSTM"},{"location":"reference/lstm/#g2p_id.lstm.LSTM","text":"Source code in g2p_id/lstm.py class LSTM : def __init__ ( self ): encoder_model_path = os . path . join ( model_path , \"encoder_model.onnx\" ) decoder_model_path = os . path . join ( model_path , \"decoder_model.onnx\" ) g2id_path = os . path . join ( model_path , \"g2id.json\" ) p2id_path = os . path . join ( model_path , \"p2id.json\" ) config_path = os . path . join ( model_path , \"config.json\" ) self . encoder = ort . InferenceSession ( encoder_model_path ) self . decoder = ort . InferenceSession ( decoder_model_path ) self . g2id = json . load ( open ( g2id_path , encoding = \"utf-8\" )) self . p2id = json . load ( open ( p2id_path , encoding = \"utf-8\" )) self . id2p = { v : k for k , v in self . p2id . items ()} self . config = json . load ( open ( config_path , encoding = \"utf-8\" )) def predict ( self , text : str ) -> str : \"\"\"Performs LSTM inference, predicting phonemes of a given word. Args: text (str): Word to convert to phonemes. Returns: str: Word in phonemes. \"\"\" input_seq = np . zeros ( ( 1 , self . config [ \"max_encoder_seq_length\" ], self . config [ \"num_encoder_tokens\" ], ), dtype = \"float32\" , ) for t , char in enumerate ( text ): input_seq [ 0 , t , self . g2id [ char ]] = 1.0 input_seq [ 0 , t + 1 :, self . g2id [ self . config [ \"pad_token\" ]]] = 1.0 encoder_inputs = { \"input_1\" : input_seq } states_value = self . encoder . run ( None , encoder_inputs ) target_seq = np . zeros ( ( 1 , 1 , self . config [ \"num_decoder_tokens\" ]), dtype = \"float32\" ) target_seq [ 0 , 0 , self . p2id [ self . config [ \"bos_token\" ]]] = 1.0 stop_condition = False decoded_sentence = \"\" while not stop_condition : decoder_inputs = { \"input_2\" : target_seq , \"input_3\" : states_value [ 0 ], \"input_4\" : states_value [ 1 ], } output_tokens , h , c = self . decoder . run ( None , decoder_inputs ) sampled_token_index = np . argmax ( output_tokens [ 0 , - 1 , :]) sampled_char = self . id2p [ sampled_token_index ] decoded_sentence += sampled_char if ( sampled_char == self . config [ \"eos_token\" ] or len ( decoded_sentence ) > self . config [ \"max_decoder_seq_length\" ] ): stop_condition = True target_seq = np . zeros ( ( 1 , 1 , self . config [ \"num_decoder_tokens\" ]), dtype = \"float32\" ) target_seq [ 0 , 0 , sampled_token_index ] = 1.0 states_value = [ h , c ] return decoded_sentence . replace ( self . config [ \"eos_token\" ], \"\" )","title":"LSTM"},{"location":"reference/lstm/#g2p_id.lstm.LSTM.predict","text":"Performs LSTM inference, predicting phonemes of a given word. Parameters: Name Type Description Default text str Word to convert to phonemes. required Returns: Type Description str Word in phonemes. Source code in g2p_id/lstm.py def predict ( self , text : str ) -> str : \"\"\"Performs LSTM inference, predicting phonemes of a given word. Args: text (str): Word to convert to phonemes. Returns: str: Word in phonemes. \"\"\" input_seq = np . zeros ( ( 1 , self . config [ \"max_encoder_seq_length\" ], self . config [ \"num_encoder_tokens\" ], ), dtype = \"float32\" , ) for t , char in enumerate ( text ): input_seq [ 0 , t , self . g2id [ char ]] = 1.0 input_seq [ 0 , t + 1 :, self . g2id [ self . config [ \"pad_token\" ]]] = 1.0 encoder_inputs = { \"input_1\" : input_seq } states_value = self . encoder . run ( None , encoder_inputs ) target_seq = np . zeros ( ( 1 , 1 , self . config [ \"num_decoder_tokens\" ]), dtype = \"float32\" ) target_seq [ 0 , 0 , self . p2id [ self . config [ \"bos_token\" ]]] = 1.0 stop_condition = False decoded_sentence = \"\" while not stop_condition : decoder_inputs = { \"input_2\" : target_seq , \"input_3\" : states_value [ 0 ], \"input_4\" : states_value [ 1 ], } output_tokens , h , c = self . decoder . run ( None , decoder_inputs ) sampled_token_index = np . argmax ( output_tokens [ 0 , - 1 , :]) sampled_char = self . id2p [ sampled_token_index ] decoded_sentence += sampled_char if ( sampled_char == self . config [ \"eos_token\" ] or len ( decoded_sentence ) > self . config [ \"max_decoder_seq_length\" ] ): stop_condition = True target_seq = np . zeros ( ( 1 , 1 , self . config [ \"num_decoder_tokens\" ]), dtype = \"float32\" ) target_seq [ 0 , 0 , sampled_token_index ] = 1.0 states_value = [ h , c ] return decoded_sentence . replace ( self . config [ \"eos_token\" ], \"\" )","title":"predict()"},{"location":"reference/lstm/#usage","text":"texts = [ \"mengembangkannya\" , \"merdeka\" , \"pecel\" , \"lele\" ] lstm = LSTM () for text in texts : print ( lstm . predict ( text )) >> m\u0259\u014b\u0259mba\u014bkan\u0272a >> m\u0259rdeka >> p\u0259t\u0283\u0259l >> lele","title":"Usage"},{"location":"reference/textprocessor/","text":"TextProcessor g2p_id.text_processor.TextProcessor Source code in g2p_id/text_processor.py class TextProcessor : def __init__ ( self ): self . measurements = {} self . thousands = [ \"ratus\" , \"ribu\" , \"juta\" , \"miliar\" , \"milyar\" , \"triliun\" ] self . months = [ \"Januari\" , \"Februari\" , \"Maret\" , \"April\" , \"Mei\" , \"Juni\" , \"Juli\" , \"Agustus\" , \"September\" , \"Oktober\" , \"November\" , \"Desember\" , ] self . measurements_path = os . path . join ( resources_path , \"measurements.tsv\" ) self . currencies_path = os . path . join ( resources_path , \"currency.tsv\" ) self . timezones_path = os . path . join ( resources_path , \"timezones.tsv\" ) with open ( self . measurements_path , \"r\" ) as file : for line in file : line = line . strip () . split ( \" \\t \" ) self . measurements [ line [ 0 ]] = line [ 1 ] self . currencies = {} with open ( self . currencies_path , \"r\" ) as file : for line in file : line = line . strip () . split ( \" \\t \" ) self . currencies [ line [ 0 ]] = line [ 1 ] self . timezones = {} with open ( self . timezones_path , \"r\" ) as file : for line in file : line = line . strip () . split ( \" \\t \" ) self . timezones [ line [ 0 ]] = line [ 1 ] self . re_thousands = \"|\" . join ([ t for t in self . thousands ]) self . re_currencies = r \"\\b\" + re . sub ( r \"\\|([^|$\u00a3\u20ac\u00a5\u20a9]+)\" , r \"| \\\\ b\\1\" , \"|\" . join ([ c for c in self . currencies ]) ) self . re_currencies = re . sub ( r \"([$\u00a3\u20ac\u00a5\u20a9])\" , r \" \\\\ \\1\" , self . re_currencies ) self . re_moneys = r \"(( {} ) ?([\\d\\.\\,]+)( ( {} )?(an)?)?)\" . format ( self . re_currencies , self . re_thousands ) self . re_measurements = \"|\" . join ([ t for t in self . measurements ]) self . re_measurements = r \"(\\b([\\d\\.\\,]+) ?( {} )\\b)\" . format ( self . re_measurements ) self . re_timezones = \"|\" . join ([ c for c in self . timezones ]) self . re_timezones = r \"((\\d{1,2})[\\.:](\\d{1,2}) \" + r \"\\b( {} )\\b)\" . format ( self . re_timezones ) self . re_http = re . compile ( r \"\"\" (https?://(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\. [a-zA-Z0-9()]{1,6}\\b[-a-zA-Z0-9()@:%_\\+.~#?&//=]*) \"\"\" , re . X ) @staticmethod def is_integer ( number ): try : int ( number ) return True except ValueError : return False @staticmethod def is_float ( number ): try : float ( number ) return True except ValueError : return False def normalize ( self , text : str ) -> str : \"\"\"Normalizes Indonesian text by expanding: - URL - Currency - Measurements - Dates - Timezones - Arabic Numerals Args: text (str): Text to normalize. Returns: str: Normalized text. \"\"\" found_errors = False # Remove URL urls = re . findall ( self . re_http , text ) for url in urls : text = text . replace ( url [ 0 ], \"\" ) # Currency moneys = re . findall ( self . re_moneys , text ) for money in moneys : number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , money [ 2 ] . strip ( \" ,.\" ))) try : if number == \"\" : continue if self . is_integer ( number ): number = int ( number ) elif self . is_float ( number ): number = float ( number ) else : number = re . sub ( r \"[.,]\" , \"\" , number ) number = int ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text . replace ( money [ 0 ] . strip ( \" ,.\" ), f \" { number } { money [ 3 ] } { self . currencies [ money [ 1 ]] } \" , ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with money: < { text } >: { number } \" ) # Measurements units = re . findall ( self . re_measurements , text ) for unit in units : number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , unit [ 1 ] . strip ( \" ,.\" ))) try : if number == \"\" : continue if re . search ( r \"\\.\" , number ): number = float ( number ) else : number = int ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text . replace ( unit [ 0 ] . strip ( \" ,.\" ), f \" { number } { self . measurements [ unit [ 2 ]] } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with measurements: < { text } >: { number } \" ) # Date dates = re . findall ( r \"(\\((\\d{1,2})/(\\d{1,2})(/(\\d+))?\\))\" , text ) for date in dates : try : day = num2words ( int ( date [ 1 ]), to = \"cardinal\" , lang = \"id\" ) month = int ( date [ 2 ]) - 1 if month >= 12 : month = 0 month = self . months [ month ] if date [ 4 ] != \"\" : year = num2words ( int ( date [ 4 ]), to = \"cardinal\" , lang = \"id\" ) date_string = f \" { day } { month } { year } \" else : date_string = f \" { day } { month } \" text = text . replace ( date [ 0 ], f \" { date_string } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with dates: < { text } >: { date } \" ) # Timezones timezones = re . findall ( self . re_timezones , text ) for timezone in timezones : try : hour = num2words ( int ( timezone [ 1 ]), to = \"cardinal\" , lang = \"id\" ) minute = num2words ( int ( timezone [ 2 ]), to = \"cardinal\" , lang = \"id\" ) zone = self . timezones [ timezone [ 3 ]] if minute == \"nol\" : time_string = f \" { hour } { zone } \" else : time_string = f \" { hour } lewat { minute } menit { zone } \" text = text . replace ( timezone [ 0 ], f \" { time_string } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with timezones: < { text } >: { timezone } \" ) # Any number re_numbers = [ r \"([\\d.,]+)\" , r \"\\d+\" ] for re_number in re_numbers : number_len = 0 for i in re . finditer ( re_number , text ): start = i . start () + number_len end = i . end () + number_len number = text [ start : end ] number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , number . strip ( \" ,.\" ))) if number == \"\" : continue if self . is_integer ( number ) or self . is_float ( number ): try : if self . is_integer ( number ): number = int ( number ) else : number = float ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text [: start ] + number + text [ end :] number_len += len ( number ) - ( end - start ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with number: < { text } >: { number } \" ) text = re . sub ( r \"\\s+\" , \" \" , text ) if found_errors : print ( f \">>> { text } \" ) return text normalize ( self , text ) Normalizes Indonesian text by expanding: URL Currency Measurements Dates Timezones Arabic Numerals Parameters: Name Type Description Default text str Text to normalize. required Returns: Type Description str Normalized text. Source code in g2p_id/text_processor.py def normalize ( self , text : str ) -> str : \"\"\"Normalizes Indonesian text by expanding: - URL - Currency - Measurements - Dates - Timezones - Arabic Numerals Args: text (str): Text to normalize. Returns: str: Normalized text. \"\"\" found_errors = False # Remove URL urls = re . findall ( self . re_http , text ) for url in urls : text = text . replace ( url [ 0 ], \"\" ) # Currency moneys = re . findall ( self . re_moneys , text ) for money in moneys : number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , money [ 2 ] . strip ( \" ,.\" ))) try : if number == \"\" : continue if self . is_integer ( number ): number = int ( number ) elif self . is_float ( number ): number = float ( number ) else : number = re . sub ( r \"[.,]\" , \"\" , number ) number = int ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text . replace ( money [ 0 ] . strip ( \" ,.\" ), f \" { number } { money [ 3 ] } { self . currencies [ money [ 1 ]] } \" , ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with money: < { text } >: { number } \" ) # Measurements units = re . findall ( self . re_measurements , text ) for unit in units : number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , unit [ 1 ] . strip ( \" ,.\" ))) try : if number == \"\" : continue if re . search ( r \"\\.\" , number ): number = float ( number ) else : number = int ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text . replace ( unit [ 0 ] . strip ( \" ,.\" ), f \" { number } { self . measurements [ unit [ 2 ]] } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with measurements: < { text } >: { number } \" ) # Date dates = re . findall ( r \"(\\((\\d{1,2})/(\\d{1,2})(/(\\d+))?\\))\" , text ) for date in dates : try : day = num2words ( int ( date [ 1 ]), to = \"cardinal\" , lang = \"id\" ) month = int ( date [ 2 ]) - 1 if month >= 12 : month = 0 month = self . months [ month ] if date [ 4 ] != \"\" : year = num2words ( int ( date [ 4 ]), to = \"cardinal\" , lang = \"id\" ) date_string = f \" { day } { month } { year } \" else : date_string = f \" { day } { month } \" text = text . replace ( date [ 0 ], f \" { date_string } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with dates: < { text } >: { date } \" ) # Timezones timezones = re . findall ( self . re_timezones , text ) for timezone in timezones : try : hour = num2words ( int ( timezone [ 1 ]), to = \"cardinal\" , lang = \"id\" ) minute = num2words ( int ( timezone [ 2 ]), to = \"cardinal\" , lang = \"id\" ) zone = self . timezones [ timezone [ 3 ]] if minute == \"nol\" : time_string = f \" { hour } { zone } \" else : time_string = f \" { hour } lewat { minute } menit { zone } \" text = text . replace ( timezone [ 0 ], f \" { time_string } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with timezones: < { text } >: { timezone } \" ) # Any number re_numbers = [ r \"([\\d.,]+)\" , r \"\\d+\" ] for re_number in re_numbers : number_len = 0 for i in re . finditer ( re_number , text ): start = i . start () + number_len end = i . end () + number_len number = text [ start : end ] number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , number . strip ( \" ,.\" ))) if number == \"\" : continue if self . is_integer ( number ) or self . is_float ( number ): try : if self . is_integer ( number ): number = int ( number ) else : number = float ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text [: start ] + number + text [ end :] number_len += len ( number ) - ( end - start ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with number: < { text } >: { number } \" ) text = re . sub ( r \"\\s+\" , \" \" , text ) if found_errors : print ( f \">>> { text } \" ) return text","title":"TextProcessor"},{"location":"reference/textprocessor/#textprocessor","text":"","title":"TextProcessor"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor","text":"Source code in g2p_id/text_processor.py class TextProcessor : def __init__ ( self ): self . measurements = {} self . thousands = [ \"ratus\" , \"ribu\" , \"juta\" , \"miliar\" , \"milyar\" , \"triliun\" ] self . months = [ \"Januari\" , \"Februari\" , \"Maret\" , \"April\" , \"Mei\" , \"Juni\" , \"Juli\" , \"Agustus\" , \"September\" , \"Oktober\" , \"November\" , \"Desember\" , ] self . measurements_path = os . path . join ( resources_path , \"measurements.tsv\" ) self . currencies_path = os . path . join ( resources_path , \"currency.tsv\" ) self . timezones_path = os . path . join ( resources_path , \"timezones.tsv\" ) with open ( self . measurements_path , \"r\" ) as file : for line in file : line = line . strip () . split ( \" \\t \" ) self . measurements [ line [ 0 ]] = line [ 1 ] self . currencies = {} with open ( self . currencies_path , \"r\" ) as file : for line in file : line = line . strip () . split ( \" \\t \" ) self . currencies [ line [ 0 ]] = line [ 1 ] self . timezones = {} with open ( self . timezones_path , \"r\" ) as file : for line in file : line = line . strip () . split ( \" \\t \" ) self . timezones [ line [ 0 ]] = line [ 1 ] self . re_thousands = \"|\" . join ([ t for t in self . thousands ]) self . re_currencies = r \"\\b\" + re . sub ( r \"\\|([^|$\u00a3\u20ac\u00a5\u20a9]+)\" , r \"| \\\\ b\\1\" , \"|\" . join ([ c for c in self . currencies ]) ) self . re_currencies = re . sub ( r \"([$\u00a3\u20ac\u00a5\u20a9])\" , r \" \\\\ \\1\" , self . re_currencies ) self . re_moneys = r \"(( {} ) ?([\\d\\.\\,]+)( ( {} )?(an)?)?)\" . format ( self . re_currencies , self . re_thousands ) self . re_measurements = \"|\" . join ([ t for t in self . measurements ]) self . re_measurements = r \"(\\b([\\d\\.\\,]+) ?( {} )\\b)\" . format ( self . re_measurements ) self . re_timezones = \"|\" . join ([ c for c in self . timezones ]) self . re_timezones = r \"((\\d{1,2})[\\.:](\\d{1,2}) \" + r \"\\b( {} )\\b)\" . format ( self . re_timezones ) self . re_http = re . compile ( r \"\"\" (https?://(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\. [a-zA-Z0-9()]{1,6}\\b[-a-zA-Z0-9()@:%_\\+.~#?&//=]*) \"\"\" , re . X ) @staticmethod def is_integer ( number ): try : int ( number ) return True except ValueError : return False @staticmethod def is_float ( number ): try : float ( number ) return True except ValueError : return False def normalize ( self , text : str ) -> str : \"\"\"Normalizes Indonesian text by expanding: - URL - Currency - Measurements - Dates - Timezones - Arabic Numerals Args: text (str): Text to normalize. Returns: str: Normalized text. \"\"\" found_errors = False # Remove URL urls = re . findall ( self . re_http , text ) for url in urls : text = text . replace ( url [ 0 ], \"\" ) # Currency moneys = re . findall ( self . re_moneys , text ) for money in moneys : number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , money [ 2 ] . strip ( \" ,.\" ))) try : if number == \"\" : continue if self . is_integer ( number ): number = int ( number ) elif self . is_float ( number ): number = float ( number ) else : number = re . sub ( r \"[.,]\" , \"\" , number ) number = int ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text . replace ( money [ 0 ] . strip ( \" ,.\" ), f \" { number } { money [ 3 ] } { self . currencies [ money [ 1 ]] } \" , ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with money: < { text } >: { number } \" ) # Measurements units = re . findall ( self . re_measurements , text ) for unit in units : number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , unit [ 1 ] . strip ( \" ,.\" ))) try : if number == \"\" : continue if re . search ( r \"\\.\" , number ): number = float ( number ) else : number = int ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text . replace ( unit [ 0 ] . strip ( \" ,.\" ), f \" { number } { self . measurements [ unit [ 2 ]] } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with measurements: < { text } >: { number } \" ) # Date dates = re . findall ( r \"(\\((\\d{1,2})/(\\d{1,2})(/(\\d+))?\\))\" , text ) for date in dates : try : day = num2words ( int ( date [ 1 ]), to = \"cardinal\" , lang = \"id\" ) month = int ( date [ 2 ]) - 1 if month >= 12 : month = 0 month = self . months [ month ] if date [ 4 ] != \"\" : year = num2words ( int ( date [ 4 ]), to = \"cardinal\" , lang = \"id\" ) date_string = f \" { day } { month } { year } \" else : date_string = f \" { day } { month } \" text = text . replace ( date [ 0 ], f \" { date_string } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with dates: < { text } >: { date } \" ) # Timezones timezones = re . findall ( self . re_timezones , text ) for timezone in timezones : try : hour = num2words ( int ( timezone [ 1 ]), to = \"cardinal\" , lang = \"id\" ) minute = num2words ( int ( timezone [ 2 ]), to = \"cardinal\" , lang = \"id\" ) zone = self . timezones [ timezone [ 3 ]] if minute == \"nol\" : time_string = f \" { hour } { zone } \" else : time_string = f \" { hour } lewat { minute } menit { zone } \" text = text . replace ( timezone [ 0 ], f \" { time_string } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with timezones: < { text } >: { timezone } \" ) # Any number re_numbers = [ r \"([\\d.,]+)\" , r \"\\d+\" ] for re_number in re_numbers : number_len = 0 for i in re . finditer ( re_number , text ): start = i . start () + number_len end = i . end () + number_len number = text [ start : end ] number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , number . strip ( \" ,.\" ))) if number == \"\" : continue if self . is_integer ( number ) or self . is_float ( number ): try : if self . is_integer ( number ): number = int ( number ) else : number = float ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text [: start ] + number + text [ end :] number_len += len ( number ) - ( end - start ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with number: < { text } >: { number } \" ) text = re . sub ( r \"\\s+\" , \" \" , text ) if found_errors : print ( f \">>> { text } \" ) return text","title":"TextProcessor"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.normalize","text":"Normalizes Indonesian text by expanding: URL Currency Measurements Dates Timezones Arabic Numerals Parameters: Name Type Description Default text str Text to normalize. required Returns: Type Description str Normalized text. Source code in g2p_id/text_processor.py def normalize ( self , text : str ) -> str : \"\"\"Normalizes Indonesian text by expanding: - URL - Currency - Measurements - Dates - Timezones - Arabic Numerals Args: text (str): Text to normalize. Returns: str: Normalized text. \"\"\" found_errors = False # Remove URL urls = re . findall ( self . re_http , text ) for url in urls : text = text . replace ( url [ 0 ], \"\" ) # Currency moneys = re . findall ( self . re_moneys , text ) for money in moneys : number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , money [ 2 ] . strip ( \" ,.\" ))) try : if number == \"\" : continue if self . is_integer ( number ): number = int ( number ) elif self . is_float ( number ): number = float ( number ) else : number = re . sub ( r \"[.,]\" , \"\" , number ) number = int ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text . replace ( money [ 0 ] . strip ( \" ,.\" ), f \" { number } { money [ 3 ] } { self . currencies [ money [ 1 ]] } \" , ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with money: < { text } >: { number } \" ) # Measurements units = re . findall ( self . re_measurements , text ) for unit in units : number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , unit [ 1 ] . strip ( \" ,.\" ))) try : if number == \"\" : continue if re . search ( r \"\\.\" , number ): number = float ( number ) else : number = int ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text . replace ( unit [ 0 ] . strip ( \" ,.\" ), f \" { number } { self . measurements [ unit [ 2 ]] } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with measurements: < { text } >: { number } \" ) # Date dates = re . findall ( r \"(\\((\\d{1,2})/(\\d{1,2})(/(\\d+))?\\))\" , text ) for date in dates : try : day = num2words ( int ( date [ 1 ]), to = \"cardinal\" , lang = \"id\" ) month = int ( date [ 2 ]) - 1 if month >= 12 : month = 0 month = self . months [ month ] if date [ 4 ] != \"\" : year = num2words ( int ( date [ 4 ]), to = \"cardinal\" , lang = \"id\" ) date_string = f \" { day } { month } { year } \" else : date_string = f \" { day } { month } \" text = text . replace ( date [ 0 ], f \" { date_string } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with dates: < { text } >: { date } \" ) # Timezones timezones = re . findall ( self . re_timezones , text ) for timezone in timezones : try : hour = num2words ( int ( timezone [ 1 ]), to = \"cardinal\" , lang = \"id\" ) minute = num2words ( int ( timezone [ 2 ]), to = \"cardinal\" , lang = \"id\" ) zone = self . timezones [ timezone [ 3 ]] if minute == \"nol\" : time_string = f \" { hour } { zone } \" else : time_string = f \" { hour } lewat { minute } menit { zone } \" text = text . replace ( timezone [ 0 ], f \" { time_string } \" ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with timezones: < { text } >: { timezone } \" ) # Any number re_numbers = [ r \"([\\d.,]+)\" , r \"\\d+\" ] for re_number in re_numbers : number_len = 0 for i in re . finditer ( re_number , text ): start = i . start () + number_len end = i . end () + number_len number = text [ start : end ] number = re . sub ( \",\" , \".\" , re . sub ( r \"\\.\" , \"\" , number . strip ( \" ,.\" ))) if number == \"\" : continue if self . is_integer ( number ) or self . is_float ( number ): try : if self . is_integer ( number ): number = int ( number ) else : number = float ( number ) number = num2words ( number , to = \"cardinal\" , lang = \"id\" ) text = text [: start ] + number + text [ end :] number_len += len ( number ) - ( end - start ) except Exception as error : found_errors = True print ( error ) print ( f \"Problem with number: < { text } >: { number } \" ) text = re . sub ( r \"\\s+\" , \" \" , text ) if found_errors : print ( f \">>> { text } \" ) return text","title":"normalize()"}]}